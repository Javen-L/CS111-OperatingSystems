NAME: Adam Young
EMAIL: youngcadam@gmail.com
ID: 000111000
SLIP DAYS: 0


####File Descriptions
Makefile: to build the deliverable program, output, graphs, and tarball.
lab2_list.csv: containing all of my results for all of the Part-2 tests.
graphs:
	- lab2b_1.png: throughput vs. number of threads for mutex and spin-lock synchronized list operations.
	- lab2b_2.png: mean time per mutex wait and mean time per operation for mutex-synchronized list operations.
	- lab2b_3.png: successful iterations vs. threads for each synchronization method.
	- lab2b_4.png: throughput vs. number of threads for mutex synchronized partitioned lists.
	- lab2b_5.png: throughput vs. number of threads for spin-lock-synchronized partitioned lists.

####Makefile
build: compile all programs 
tests: run all (over 200) specified test cases to generate results in CSV files
graphs: use gnuplot(1) and the created data reduction scripts to generate the required graphs
dist: create the deliverable tarball
clean: delete all programs and output created by the Makefile


####Resources
gperftools:
	-https://gperftools.github.io/gperftools/cpuprofile.html	
	-https://stackoverflow.com/questions/24086867

gnuplot:
	-http://gnuplot.sourceforge.net/docs_4.2/node157.html

####Questions

######2.3.1
Where do you believe most of the CPU time is spent in the 1 and 2-thread list tests ?
I believe most of the CPU time spent during the 1 and 2-thread list tests is spent on 
the list operations (insert, length, lookup, delete).

Why do you believe these to be the most expensive parts of the code?
I believe these to be the most expensive parts of the code because there are a low
number of threads which means threads spend less time waiting and there are less 
synchronization operations being performed.

Where do you believe most of the CPU time is being spent in the high-thread spin-lock tests?
For high-thread spin-lock tests, I believe most of the time is being spent on spin-waiting for
other threads to give up the lock.

Where do you believe most of the CPU time is being spent in the high-thread mutex tests?
For high-thread mutex tests, I believe most of the time is being spent on list operations if
the iteration count is high, but if the number of iterations is low, we wouldn't benefit
from parallelism as much so I think the majority of time would be spent on lock operations
and scheduling threads that are blocked/sleeping.

######2.3.2
Where (what lines of code) are consuming most of the CPU time when the spin-lock version of the list exerciser is run with a large number of threads?
The lines of code pertaining to spin-waiting are the most expensive. This is also what we 
observe in profile.out:
     .      .   78: 	while (__sync_lock_test_and_set(&spinlock, 1))
   619    619   79: 	  while(spinlock); //spin until we can get the lock


Why does this operation become so expensive with large numbers of threads?
With large numbers of threads, many threads compete to get the lock so many threads waste 
CPU cycles spin-waiting for other threads.

######2.3.3
Why does the average lock-wait time rise so dramatically with the number of contending threads?
Why does the completion time per operation rise (less dramatically) with the number of
contending threads?
How is it possible for the wait time per operation to go up faster (or higher) than the 
completion time per operation?

We have many threads competing to get the lock so that they can execute the critical section.
The difference in this scenario is that we don't spend time spin-waiting, instead, the thread
is just put in a blocked state until another thread releases the lock.

The threads are placed into a blocked state while waiting for access to a shared resource.
Since there is only one resource (unlike with --lists below), as the number of contending
threads increases, the number of threads competing and waiting to access the mutex lock
which leads to higher wait times for all threads because each thread now has a lower 
probability of getting the lock (combined, these factors leads to dramatic increase in
wait time). 

The completion time per operation rises (less dramatically) with the number of
contending threads because we have many threads competing for access to one shared resource,
however, we do gain a slight efficiency boost for having the threads work in parallel.

It is possible for the wait time per operation to go up faster than the completion time per
operation because we have more threads waiting than we do working 


######2.3.4
Explain the change in performance of the synchronized methods as a function of the number of 
lists.
Should the throughput continue increasing as the number of lists is further increased? If not,
explain why not.
It seems reasonable to suggest the throughput of an N-way partitioned list should be 
equivalent to the throughput of a single list with fewer (1/N) threads. Does this appear to 
be true in the above curves? If not, explain why not.

There are multiple distinct lists which have their own locks and critical sections. Now 
more than one critical section can execute in parallel because there can be as many locks as
there are lists which are distributed amongst different threads.
Yes, the throughput should continue increasing until we reach the number of cores on the CPU
so that each thread could execute on each core. After we reach that point, then the throughput
of an N-way partitioned list actually begins to degrade because more time is spent on memory
access and scheduling overheads.